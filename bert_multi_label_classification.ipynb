{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom pathlib import Path\nfrom typing import *\n\nimport torch\nimport torch.optim as optim","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from fastai import *\nfrom fastai.vision import *\nfrom fastai.text import *\nfrom fastai.callbacks import *","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%bash\npip install pytorch-pretrained-bert","execution_count":3,"outputs":[{"output_type":"stream","text":"Collecting pytorch-pretrained-bert\n  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\nRequirement already satisfied: torch>=0.4.1 in /opt/conda/lib/python3.6/site-packages (from pytorch-pretrained-bert) (1.0.1.post2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from pytorch-pretrained-bert) (1.16.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from pytorch-pretrained-bert) (2.21.0)\nRequirement already satisfied: boto3 in /opt/conda/lib/python3.6/site-packages (from pytorch-pretrained-bert) (1.9.134)\nRequirement already satisfied: regex in /opt/conda/lib/python3.6/site-packages (from pytorch-pretrained-bert) (2019.4.14)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from pytorch-pretrained-bert) (4.31.1)\nRequirement already satisfied: urllib3<1.25,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (1.22)\nRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (2.6)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (2019.3.9)\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (3.0.4)\nRequirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /opt/conda/lib/python3.6/site-packages (from boto3->pytorch-pretrained-bert) (0.2.0)\nRequirement already satisfied: botocore<1.13.0,>=1.12.134 in /opt/conda/lib/python3.6/site-packages (from boto3->pytorch-pretrained-bert) (1.12.134)\nRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3->pytorch-pretrained-bert) (0.9.4)\nRequirement already satisfied: docutils>=0.10 in /opt/conda/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.134->boto3->pytorch-pretrained-bert) (0.14)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /opt/conda/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.134->boto3->pytorch-pretrained-bert) (2.6.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.134->boto3->pytorch-pretrained-bert) (1.12.0)\nInstalling collected packages: pytorch-pretrained-bert\nSuccessfully installed pytorch-pretrained-bert-0.6.2\n","name":"stdout"},{"output_type":"stream","text":"You are using pip version 19.0.3, however version 20.2.1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Config(dict):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        for k, v in kwargs.items():\n            setattr(self, k, v)\n    \n    def set(self, key, val):\n        self[key] = val\n        setattr(self, key, val)\n\nconfig = Config(\n    testing=False,\n    bert_model_name=\"bert-base-uncased\",\n    max_lr=3e-5,\n    epochs=4,\n    use_fp16=True,\n    bs=8,\n    discriminative=False,\n    max_seq_len=256,\n)","execution_count":4,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from pytorch_pretrained_bert import BertTokenizer\nbert_tok = BertTokenizer.from_pretrained(\n    config.bert_model_name,\n)","execution_count":5,"outputs":[{"output_type":"stream","text":"100%|██████████| 231508/231508 [00:00<00:00, 2571216.93B/s]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def _join_texts(texts:Collection[str], mark_fields:bool=False, sos_token:Optional[str]=BOS):\n    \"\"\"Borrowed from fast.ai source\"\"\"\n    if not isinstance(texts, np.ndarray): texts = np.array(texts)\n    if is1d(texts): texts = texts[:,None]\n    df = pd.DataFrame({i:texts[:,i] for i in range(texts.shape[1])})\n    text_col = f'{FLD} {1} ' + df[0].astype(str) if mark_fields else df[0].astype(str)\n    if sos_token is not None: text_col = f\"{sos_token} \" + text_col\n    for i in range(1,len(df.columns)):\n        #text_col += (f' {FLD} {i+1} ' if mark_fields else ' ') + df[i]\n        text_col += (f' {FLD} {i+1} ' if mark_fields else ' ') + df[i].astype(str)\n    return text_col.values","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class FastAiBertTokenizer(BaseTokenizer):\n    \"\"\"Wrapper around BertTokenizer to be compatible with fast.ai\"\"\"\n    def __init__(self, tokenizer: BertTokenizer, max_seq_len: int=128, **kwargs):\n        self._pretrained_tokenizer = tokenizer\n        self.max_seq_len = max_seq_len\n\n    def __call__(self, *args, **kwargs):\n        return self\n\n    def tokenizer(self, t:str) -> List[str]:\n        \"\"\"Limits the maximum sequence length\"\"\"\n        return [\"[CLS]\"] + self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2] + [\"[SEP]\"]","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nimport pandas as pd\ntrain, test = [pd.read_csv('/kaggle/input/sentiment-actual-detection/'+ fname) for fname in [\"train.csv\", \"val.csv\"]]\nval = test # we won't be using a validation set but you can easily create one using train_test_split\ntrain = train.dropna()\ntest = test.dropna()\nval = val.dropna()","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if config.testing:\n    train = train.head(1024)\n    val = val.head(1024)\n    test = test.head(1024)","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fastai_bert_vocab = Vocab(list(bert_tok.vocab.keys()))","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fastai_tokenizer = Tokenizer(tok_func=FastAiBertTokenizer(bert_tok, max_seq_len=config.max_seq_len), pre_rules=[], post_rules=[])","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_cols = ['tyre agedot code positive', 'advisoragent service negative', 'discount not applied positive', \n                                     'discount not applied negative', 'response time negative', 'wait time negative', \n                                     'mobile fitter negative', 'refund timescale negative', 'length of fitting positive', \n                                     'no stock negative', 'refund not actioned positive', 'booking confusion positive', \n                                     'delivery punctuality negative', 'tyre age/dot code negative', 'no stock positive', \n                                     \"mobile fitter didn't arrive negative\", 'facilities negative', 'value for money negative', \n                                     'discounts negative', 'change of time positive', 'failed payment positive', \n                                     'incorrect tyres sent positive', 'refund timescale positive', 'failed payment negative',\n                                     'call wait time positive', 'wait time positive', 'damage negative', 'tyre quality positive', \n                                     'ease of booking positive', 'change of time negative', \"mobile fitter didn't arrive positive\",\n                                     'change of date negative', 'advisor/agent service negative', 'extra charges negative', \n                                     'location positive', 'late notice negative', 'discounts positive', 'garage service negative', \n                                     'tyre quality negative', 'response time positive', 'booking confusion negative',\n                                     'delivery punctuality positive', 'advisoragent service positive', 'refund positive', \n                                     'refund negative', 'ease of booking negative', 'garage service positive', 'location negative', \n                                     'balancing negative', 'facilities positive', 'call wait time negative', 'advisor/agent service positive',\n                                     'tyre agedot code negative', 'mobile fitter positive', 'late notice positive', \n                                     'incorrect tyres sent negative', 'value for money positive', 'extra charges positive', \n                                     'balancing positive', 'length of fitting negative', 'refund not actioned negative', 'change of date positive']\n\n# databunch = TextDataBunch.from_df(\".\", train, val, test,\n#                   tokenizer=fastai_tokenizer,\n#                   vocab=fastai_bert_vocab,\n#                   include_bos=False,\n#                   include_eos=False,\n#                   text_cols=\"comment_text\",\n#                   label_cols=label_cols,\n#                   bs=config.bs,\n#                   collate_fn=partial(pad_collate, pad_first=False, pad_idx=0),\n#              )","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Alternatively, we can pass our own list of Preprocessors to the databunch (this is effectively what is happening behind the scenes)"},{"metadata":{"trusted":true},"cell_type":"code","source":"class BertTokenizeProcessor(TokenizeProcessor):\n    def __init__(self, tokenizer):\n        super().__init__(tokenizer=tokenizer, include_bos=False, include_eos=False)\n\nclass BertNumericalizeProcessor(NumericalizeProcessor):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, vocab=Vocab(list(bert_tok.vocab.keys())), **kwargs)\n\ndef get_bert_processor(tokenizer:Tokenizer=None, vocab:Vocab=None):\n    \"\"\"\n    Constructing preprocessors for BERT\n    We remove sos/eos tokens since we add that ourselves in the tokenizer.\n    We also use a custom vocabulary to match the numericalization with the original BERT model.\n    \"\"\"\n    return [BertTokenizeProcessor(tokenizer=tokenizer),\n            NumericalizeProcessor(vocab=vocab)]","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class BertDataBunch(TextDataBunch):\n    @classmethod\n    def from_df(cls, path:PathOrStr, train_df:DataFrame, valid_df:DataFrame, test_df:Optional[DataFrame]=None,\n                tokenizer:Tokenizer=None, vocab:Vocab=None, classes:Collection[str]=None, text_cols:IntsOrStrs=1,\n                label_cols:IntsOrStrs=0, label_delim:str=None, **kwargs) -> DataBunch:\n        \"Create a `TextDataBunch` from DataFrames.\"\n        p_kwargs, kwargs = split_kwargs_by_func(kwargs, get_bert_processor)\n        # use our custom processors while taking tokenizer and vocab as kwargs\n        processor = get_bert_processor(tokenizer=tokenizer, vocab=vocab, **p_kwargs)\n        if classes is None and is_listy(label_cols) and len(label_cols) > 1: classes = label_cols\n        src = ItemLists(path, TextList.from_df(train_df, path, cols=text_cols, processor=processor),\n                        TextList.from_df(valid_df, path, cols=text_cols, processor=processor))\n        src = src.label_for_lm() if cls==TextLMDataBunch else src.label_from_df(cols=label_cols, classes=classes)\n        if test_df is not None: src.add_test(TextList.from_df(test_df, path, cols=text_cols))\n        return src.databunch(**kwargs)","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# this will produce a virtually identical databunch to the code above\ndatabunch = BertDataBunch.from_df(\".\", train, val, test,\n                  tokenizer=fastai_tokenizer,\n                  vocab=fastai_bert_vocab,\n                  text_cols=\"text\",\n                  label_cols=label_cols,\n                  bs=config.bs,\n                  collate_fn=partial(pad_collate, pad_first=False, pad_idx=0),\n             )","execution_count":15,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pytorch_pretrained_bert.modeling import BertConfig, BertForSequenceClassification\nbert_model = BertForSequenceClassification.from_pretrained(config.bert_model_name, num_labels=62)","execution_count":16,"outputs":[{"output_type":"stream","text":"100%|██████████| 407873900/407873900 [00:09<00:00, 43058459.54B/s]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_func = nn.BCEWithLogitsLoss()","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from fastai.callbacks import *\n\nlearner = Learner(\n    databunch, bert_model,\n    loss_func=loss_func,\n)\nif config.use_fp16: learner = learner.to_fp16()","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#learner.lr_find()","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learner.recorder.plot()","execution_count":20,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"'Learner' object has no attribute 'recorder'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-30b7e7ccdb87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mAttributeError\u001b[0m: 'Learner' object has no attribute 'recorder'"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learner.fit_one_cycle(config.epochs, max_lr=config.max_lr)","execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>epoch</th>\n      <th>train_loss</th>\n      <th>valid_loss</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.082287</td>\n      <td>0.085299</td>\n      <td>01:33</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0.061824</td>\n      <td>0.067664</td>\n      <td>01:34</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.048695</td>\n      <td>0.062220</td>\n      <td>01:35</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.048452</td>\n      <td>0.061246</td>\n      <td>01:32</td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"learner.validate()","execution_count":22,"outputs":[{"output_type":"execute_result","execution_count":22,"data":{"text/plain":"[0.06124604]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_preds_as_nparray(ds_type) -> np.ndarray:\n    \"\"\"\n    the get_preds method does not yield the elements in order by default\n    we borrow the code from the RNNLearner to resort the elements into their correct order\n    \"\"\"\n    preds = learner.get_preds(ds_type)[0].detach().cpu().numpy()\n    sampler = [i for i in databunch.dl(ds_type).sampler]\n    reverse_sampler = np.argsort(sampler)\n    return preds[reverse_sampler, :]","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds = get_preds_as_nparray(DatasetType.Test)\nprint(test_preds.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_labels = val[label_cols]\nval_label_array = np.zeros((0,62))\nfor (key,value) in val_labels.iterrows():\n    \n    val_array = np.expand_dims(np.array(list(value)),0)\n    val_label_array = np.append(val_label_array,val_array, axis = 0)\n\nprint(val_label_array.shape)\nerror = 0\nfor index in range(val_label_array.shape[0]):\n    mse = sum((test_preds[index,:]-val_label_array[index,:])**2)/ sum(val_label_array[index,:]**2)\n    #print(mse)\n    error +=mse\nprint('The error in predictions:{}'.format(error/val_label_array.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission = pd.read_csv(DATA_ROOT / \"sample_submission.csv\")\nif config.testing: sample_submission = sample_submission.head(test.shape[0])\nsample_submission[label_cols] = test_preds\nsample_submission.to_csv(\"predictions.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}